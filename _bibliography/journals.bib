@article{grano2019scented,
title = "Scented Since the Beginning: On the Diffuseness of Test Smells in Automatically Generated Test Code",
journal = "Journal of Systems and Software",
month = 7,
year = "2019",
issn = "0164-1212",
doi = "https://doi.org/10.1016/j.jss.2019.07.016",
url = "http://www.sciencedirect.com/science/article/pii/S0164121219301487",
author = {Grano, Giovanni and Palomba, Fabio and Di Nucci, Dario and De Lucia, Andrea and Gall, Harald},
keywords = "Test Smells, Test Case Generation, Software Quality, Empirical Studies",
abstract = "Software testing represents a key software engineering practice to ensure source code quality and reliability. To support developers in this activity and reduce testing effort, several automated unit test generation tools have been proposed. Most of these approaches have the main goal of covering as more branches as possible. While these approaches have good performance, little is still known on the maintainability of the test code they produce, i.e., whether the generated tests have a good code quality and if they do not possibly introduce issues threatening their effectiveness. To bridge this gap, in this paper we study to what extent existing automated test case generation tools produce potentially problematic test code. We consider seven test smells, i.e., suboptimal design choices applied by programmers during the development of test cases, as measure of code quality of the generated tests, and evaluate their diffuseness in the unit test classes automatically generated by three state-of-the-art tools such as Randoop, JTExpert, and Evosuite. Moreover, we investigate whether there are characteristics of test and production code influencing the generation of smelly tests. Our study shows that all the considered tools tend to generate a high quantity of two specific test smell types, i.e., Assertion Roulette and Eager Test, which are those that previous studies showed to negatively impact the reliability of production code. We also discover that test size is correlated with the generation of smelly tests. Based on our findings, we argue that more effective automated generation algorithms that explicitly take into account test code quality should be further investigated and devised.",
pdf="journals/jss-smells.pdf"
}

@article{grano2019branch,
  author    = {Giovanni Grano and
               Timofey V. Titov and
               Sebastiano Panichella and
               Harald C. Gall},
  title     = {Branch Coverage Prediction in Automated Testing},
  journal   = {Journal of Software: Evolution and Process},
  year      = {2019},
  month = 1,
  url       = {https://10.1002/smr.2158},
  doi       = {10.1002/smr.2158},
  url = {https://doi.org/10.1002/smr.2158},
  abstract  = {Software testing is crucial in continuous integration (CI).
Ideally, at every commit, all the test cases should be executed and, moreover, new test cases should be generated for the new source code.
This is especially true in a Continuous Test Generation (CTG) environment, where the automatic generation of test cases is integrated into the continuous integration pipeline.
In this context, developers want to achieve a certain minimum level of coverage for every software build.
However, executing all the test cases and, moreover, generating new ones for all the classes at every commit is not feasible. 
As a consequence, developers have to select which subset of classes has to be tested and/or targeted by test-case generation.
We argue that knowing a priori the branch-coverage that can be achieved with test-data generation tools can help developers into taking informed-decision about those issues. 
In this paper, we investigate the possibility to use source-code metrics
to predict the coverage achieved by test-data generation tools.
We use four different categories of source-code features and assess the prediction on a large dataset involving more than 3'000 Java classes.
We compare different machine learning algorithms and conduct a fine-grained feature analysis aimed at investigating the factors that most impact the prediction accuracy.
Moreover, we extend our investigation to four different search-budgets.
Our evaluation shows that the best model achieves an average 0.15 and 0.21 MAE on nested cross-validation over the different budgets, respectively on EvoSuite and Randoop. Finally, the discussion of the results demonstrate the relevance of coupling-related features for the prediction accuracy.},
pdf = {journals/jsep19.pdf}
}

@article{sorbo2020investigating,
  author    = {Andrea {Di Sorbo} and Giovanni Grano and
               Corrado Aron Visaggio and
               Sebastiano Panichella},
  title     = {Investigating the Criticality of User Reported Issues through their Relations with App Rating},
  journal   = {Journal of Software: Evolution and Process},
  year      = {2020},
  month = 8,
  abstract  = {App quality impacts user experience and satisfaction. As consequence, both app ratings and user feedback reported in app reviews are directly influenced by the users perceived app quality. We conjecture that to perform an effective maintenance and evolution of mobile applications, it is crucial to find ways to detect the user reported issues that most impact the app rating (i.e., the app success). In this paper, we experiment the combined usage of app rating and  user reviews analysis (i) to investigate the most important factors influencing the perceived app quality, (ii) focusing on the topics discussed in user review that most relate with app rating. In addition, we investigate whether specific code quality metrics could be monitored in order to prevent the rising of reviews with low ratings. Through an empirical study involving 210,517 reviews related to 317 Android apps, we first investigate the particular types of user feedback (e.g., bugs- or feature-related issues) that are associated with reviews with high/low rates. Then, we  observe the extent to which (issue) metrics based on app rating and user reviews analysis correlate with specific code/mobile quality indicators. Our study demonstrates that user feedback reporting bugs are negatively correlated with the rating, while user reviews reporting feature requests do not. Interestingly, depending on the app category, we observed that different kinds of issues have rather different relationships with the rating and the user perceived quality of the app. In addition, we observe that for specific app categories (e.g., Communication) some code quality factors (e.g., the Android specific ones) have significant relationships with the raising of certain types of feedback, that, in turn, are negatively connected with app ratings. Our work complements state-of-art approaches  that leverage app rating  to measure user satisfaction and the (perceived) app software quality. Moreover, it demonstrates how an analysis based on the app rating can be  enriched by a context-based analysis (i.e., taking into account the specific nature) of the different apps and a contextual-based user review analysis, this to  guide developers better understanding user needs and achieve higher app success.}
}

@article{grano2019testingfewer,
  title={Testing with Fewer Resources: An Adaptive Approach to Performance-Aware Test Case Generation},
  author={Grano, Giovanni and Laaber, Christoph and Panichella, Annibale and Panichella, Sebastiano},
  journal={IEEE Transactions on Software Engineering},
  year={2019},
  month = 10,
  doi = {10.1109/TSE.2019.2946773},
  url = {http://doi.org/10.1109/TSE.2019.2946773},
  publisher={IEEE},
  abstract = {Automated test case generation is an effective technique to yield high-coverage test suites. While the majority of research effort has been devoted to satisfying coverage criteria, a recent trend emerged towards optimizing other non-coverage aspects. In this regard, runtime and memory usage are two essential dimensions: less expensive tests reduce the resource demands for the generation process and later regression testing phases. This study shows that performance-aware test case generation requires solving two main challenges: providing a good approximation of resource usage with minimal overhead and avoiding detrimental effects on both final coverage and fault detection effectiveness. To tackle these challenges, we conceived a set of performance proxies ---inspired by previous work on performance testing--- that provide a reasonable estimation of the test execution costs (i.e., runtime and memory usage). Thus, we propose an adaptive strategy, called aDynaMOSA, which leverages these proxies by extending DynaMOSA, a state-of-the-art evolutionary algorithm in unit testing. Our empirical study ---involving 110 non-trivial Java classes--- reveals that our adaptive approach generates test suite with statistically significant improvements in runtime (-25%) and heap memory consumption (-15%) compared to DynaMOSA. Additionally, aDynaMOSA has comparable results to DynaMOSA over seven different coverage criteria and similar fault detection effectiveness. Our empirical investigation also highlights that the usage of performance proxies (i.e., without the adaptiveness) is not sufficient to generate more performant test cases without compromising the overall coverage.},
  pdf={journals/tse-adynamosa.pdf},
  slides={slides/icse-tse.pdf}
}

@article{grano2019lightweight,
  title={Lightweight Assessment of Test-Case Effectiveness using Source-Code-Quality Indicators},
  author={Grano, Giovanni and Palomba, Fabio and Gall, Harald},
  journal={IEEE Transactions on Software Engineering},
  year={2019},
  month = 2,
  doi = {10.1109/TSE.2019.2903057},
  url = {https://doi.org/10.1109/TSE.2019.2903057},
  publisher={IEEE},
  abstract = {Test cases are crucial to help developers preventing the introduction of software faults. Unfortunately, not all the tests are properly designed or can effectively capture faults in production code. Some measures have been defined to assess test-case effectiveness: the most relevant one is the mutation score, which highlights the quality of a test by generating the so-called mutants, ie variations of the production code that make it faulty and that the test is supposed to identify. However, previous studies revealed that mutation analysis is extremely costly and hard to use in practice. The approaches proposed by researchers so far have not been able to provide practical gains in terms of mutation testing efficiency. This leaves the problem of efficiently assessing test-case effectiveness as still open. In this paper, we investigate a novel, orthogonal, and lightweight methodology to assess test-case effectiveness: in particular, we study the feasibility to exploit production and test-code-quality indicators to estimate the mutation score of a test case. We firstly select a set of 67 factors and study their relation with test-case effectiveness. Then, we devise a mutation score estimation model exploiting such factors and investigate its performance as well as its most relevant features. The key results of the study reveal that our estimation model only based on static features has 86% of both F-Measure and AUC-ROC. This means that we can estimate the test-case effectiveness, using source-code-quality indicators, with high accuracy and without executing the tests. As a consequence, we can provide a practical approach that is beyond the typical limitations of current mutation testing techniques.},
  pdf={journals/tse19lightweight.pdf},
  slides={slides/ase19jf.pdf}
}

@article{vassallo2019large,
  title={A Large-Scale Empirical Exploration on Refactoring Activities in Open Source Software Projects},
  author={Vassallo, Carmine and Grano, Giovanni and Palomba, Fabio and Gall, Harald and Bacchelli, Alberto},
  journal={Science of Computer Programming},
  year={2019},
  month = 5,
  publisher={Elsevier},
  abstract = {Refactoring is a well-established practice that aims at improving the internal structure of a software system without changing its external behavior. Existing literature provides evidence of how and why developers perform refactoring in practice. In this paper, we continue on this line of research by performing a large-scale empirical analysis of refactoring practices in 200 open source systems. Specifically, we analyze the change history of these systems at commit level to investigate: (i) whether developers perform refactoring operations and, if so, which are more diffused and (ii) when refactoring operations are applied, and (iii) which are the main developer-oriented factors leading to refactoring. 
    Based on our results, future research can focus on enabling automatic support for less frequent refactorings and on recommending refactorings based on the developer's workload, project's maturity and developer's commitment to the project.},
  pdf={journals/scico19.pdf}
}
