@inproceedings{grano2018anempirical,
  author = {Grano, Giovanni and Scalabrino, Simone and Oliveto, Rocco and Gall, Harald},
  title = {An Empirical Investigation on the Readability of Manual and Generated Test Cases},
  booktitle = {Proceedings of the 26th International Conference on Program Comprehension, ICPC},
  year = {2018},
  organization={IEEE Press},
  abstract = {Software testing is one of the most crucial tasks in the typical development process. Developers are usually required to write unit test cases for the code they implement. Since this is a time-consuming task, in last years many approaches and tools for automatic test case generation — such as EvoSuite — have been introduced. Nevertheless, developers have to maintain and evolve tests to sustain the changes in the source code; therefore, having readable test cases is important to ease such a process.
  However, it is still not clear whether developers make an effort in writing readable unit tests. Therefore, in this paper, we conduct an explorative study comparing the readability of manually written test cases with the classes they test. Moreover, we deepen such analysis looking at the readability of automatically generated test cases. Our results suggest that developers tend to neglect the readability of test cases and that automatically generated test cases are generally even less readable than manually written ones.},
  pdf = {conf/icpc18short.pdf},
  slides = {slides/icpc18.pdf}
} 

@inproceedings{pelloni2018becloma,
  author    = {Pelloni, Lucas and
               Grano, Giovanni and
               Ciurumelea, Adelina and
               Palomba, Fabio and
               Panichella, Sebastiano and
               Gall, Harald},
  title     = {BECLoMA: Augmenting Stack Traces with User Review Information},
  booktitle={Software Analysis, Evolution and Reengineering (SANER), 2018 IEEE 25th International Conference on},
  year      = {2018},
  abstract  = {Mobile devices such as smartphones, tablets and wearables are changing the way we do things, radically modifying our approach to technology. To sustain the brutal competition in the mobile market, developers need to deliver high quality applications in a short release cycle. 
Therefore, to maximize their market success, they aim to reveal and fix bugs as soon as possible.
For this reason, researchers and practitioners proposed testing tools to automate the process of bug discovery and fixing. 
In the mobile development context, the content of user reviews represents an unmatched source for developers seeking for defects in their applications. However, no prior work explored the adoption of information available in user reviews for testing purposes. 
In this demo we present BECLoMA, a tool to enable the integration of user feedback in the testing process of mobile apps.
BECLoMA links information from testing tools and user reviews, presenting developers an augmented testing report combining stack traces with user reviews information referring to the same crash. 
We show that BECLoMA facilitates not only the diagnosis and fix of app bugs, but also presents additional benefits: it eases the usage of testing tools and automates the analysis of user reviews from the Google Play Store.},
  pdf = {tool/saner18demo.pdf},
  slides = {slides/saner18tool.pdf}
}

@inproceedings{grano2018howhight,
  author    = {Grano, Giovanni and
               Timov, Timofey  and
               Panichella, Sebastiano and
               Gall, Harald},
  title     = {How High Will It Be? Using Machine Learning Models to Predict Branch Coverage in Automated Testing},
  booktitle = {Machine Learning Techniques for Software Quality Evaluation (MaLTeSQuE), IEEE Workshop on},
  year      = {2018},
  abstract  = {Software testing is a crucial component in modern continuous integration development environment.
Ideally, at every commit, all the system's test cases should be executed and moreover, new test cases should be generated for the new code.
This is especially true in the a Continuous Test Generation (CTG) environment, where the automatic generation of test cases is integrated into the continuous integration pipeline.
Furthermore, developers want to achieve a minimum level of coverage for every build of their systems.
Since both executing all the test cases and generating new ones for all the classes at every commit is not feasible, they have to select which subset of classes has to be tested.
In this context, knowing a priori the branch coverage that can be achieved with test data generation tools might gives some useful indications for answering such a question.
In this paper, we take the first steps towards the definition of machine learning models to predict the branch coverage achieved by test data generation tools.
We conduct a preliminary study considering well known code metrics as a features.
Despite the simplicity of these features, our results show that using machine learning to predict branch coverage in automated testing is a viable and feasible option.},
  pdf = {workshop/maltesque.pdf},
  slides = {slides/maltesque.pdf}
}

@inproceedings{grano2017android,
  author    = {Grano, Giovanni and
               {Di Sorbo}, Andrea and
               Mercaldo, Francesco and
               Visaggio, {Corrado Aaron}  and
               Canfora, Gerardo and
               Panichella, Sebastiano},
  title     = {Android apps and user feedback: a dataset for software evolution and
               quality improvement},
  booktitle = {WAMA@ESEC/SIGSOFT {FSE}},
  abstract  = {Nowadays, Android represents the most popular mobile platform with a market share of around 80%. Previous research showed that data contained in user reviews and code change history of mobile apps represent a rich source of information for reducing software maintenance and development effort, increasing customers' satisfaction. Stemming from this observation, we present in this paper a large dataset of Android applications belonging to 23 different apps categories, which provides an overview of the types of feedback users report on the apps and documents the evolution of the related code metrics. The dataset contains about 395 applications of the F-Droid repository, including around 600 versions, 280,000 user reviews and more than 450,000 user feedback (extracted with specific text mining approaches). Furthermore, for each app version in our dataset, we employed the Paprika tool and developed several Python scripts to detect 8 different code smells and compute 22 code quality indicators. The paper discusses the potential usefulness of the dataset for future research in the field.},
  pdf       = {workshop/wama17.pdf},
  slides    = {slides/wama17.pdf},
  pages     = {8--11},
  publisher = {{ACM}},
  year      = {2017}
}




@inproceedings{grano2018,
  author    = {Grano, Giovanni and
               Ciurumelea, Adelina and
               Palomba, Fabio and
               Panichella, Sebastiano and
               Gall, Harald},
  title     = {Exploring the Integration of User Feedback in Automated Testing of Android Applications},
  booktitle={Software Analysis, Evolution and Reengineering (SANER), 2018 IEEE 25th International Conference on},
  year      = {2018},
  abstract  = {The intense competition characterizing mobile application's marketplaces forces developers to create and maintain high-quality mobile apps in order to ensure their commercial success and acquire new users. This motivated the research community to propose solutions that automate the testing process of mobile apps. However, the main problem of current testing tools is that they generate redundant and random inputs that are insufficient to properly simulate the human behavior, thus leaving feature and crash bugs undetected until they are encountered by users. To cope with this problem, we conjecture that information available in user reviews---that previous work showed as effective for maintenance and evolution problems---can be successfully exploited to identify the main issues users experience while using mobile applications, e.g., GUI problems and crashes. 
In this paper we provide initial insights into this direction, investigating (i) what type of user feedback can be actually exploited for testing purposes, (ii) how complementary user feedback and automated testing tools are, when detecting crash bugs or errors and (iii) whether an automated system able to monitor crash-related information reported in user feedback is sufficiently accurate. Results of our study, involving 11,296 reviews of 8 mobile applications, show that user feedback can be exploited to provide contextual details about errors or exceptions detected by automated testing tools. Moreover, they also help detecting bugs that would remain uncovered when rely on testing tools only. Finally, the accuracy of the proposed automated monitoring system demonstrates the feasibility of our vision, i.e., integrate user feedback into testing process.},
    pdf       = {conf/saner18.pdf},
    slides = {slides/saner18.pdf}
}

@inproceedings{scalabrino2016search,
  author    = {Scalabrino, Simone and
               Grano, Giovanni and
              {Di Nucci}, Dario and
              Oliveto, Rocco and
               {De Lucia}, Andrea},
  title     = {Search-Based Testing of Procedural Programs: Iterative Single-Target
               or Multi-target Approach?},
  booktitle={International Symposium on Search Based Software Engineering},
  pages={64--79},
  year={2016},
  organization={Springer},
  abstract  = {In the context of testing of Object-Oriented (OO) software systems, researchers have recently proposed search based approaches to automatically generate whole test suites by considering simultaneously all targets (e.g., branches) defined by the coverage criterion (multi-target approach). The goal of whole suite approaches is to overcome the problem of wasting search budget that iterative single-target approaches (which iteratively generate test cases for each target) can encounter in case of infeasible targets. However, whole suite approaches have not been implemented and experimented in the context of procedural programs. In this paper we present OCELOT (Optimal Coverage sEarch-based tooL for sOftware Testing), a test data generation tool for C programs which implements both a state-of-the-art whole suite approach and an iterative single-target approach designed for a parsimonious use of the search budget. We also present an empirical study conducted on 35 open-source C programs to compare the two approaches implemented in OCELOT. The results indicate that the iterative single-target approach provides a higher efficiency while achieving the same or an even higher level of coverage than the whole suite approach.},
  volume    = {9962},
  slides    = {slides/ocelot.pdf},
  pdf       = {conf/lips.pdf}
}
